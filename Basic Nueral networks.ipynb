{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "NQpoApRIgqyc",
        "outputId": "48c7bad7-3423-4260-aaf1-008a3434da79"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-334d6baeba70>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#defining an empty list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'C:/Users/Prana/OneDrive/Documents/Nueral Networks FA23 (Grad)/Assignment1_data'\u001b[0m \u001b[0;31m#assigning the path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtxt_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get a list of the txt files in the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtxt_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;31m# Open the TXT file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Prana/OneDrive/Documents/Nueral Networks FA23 (Grad)/Assignment1_data'"
          ]
        }
      ],
      "source": [
        "#importing all necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import seaborn as sns\n",
        "\n",
        "t=[] #defining an empty list\n",
        "path='C:/Users/Prana/OneDrive/Documents/Nueral Networks FA23 (Grad)/Assignment1_data' #assigning the path\n",
        "txt_files = os.listdir(path) #get a list of the txt files in the directory\n",
        "for txt_file in txt_files:\n",
        "  # Open the TXT file\n",
        "  with open(os.path.join(path, txt_file), \"r\") as f:\n",
        "    # Read the contents of the TXT file or document\n",
        "    tf = f.read()\n",
        "    t.append(tf) #append the document to the list to make a list of documents\n",
        "doc = [re.sub(\"\\n\", \"\", string) for string in t] #Remove the newline characters from the strings in each document\n",
        "print(doc) #print the list of documents\n",
        "\n",
        "#CREATING WORD DOCUMENT MATRIX BASED ON LIST OF DOCUMENTS\n",
        "\n",
        "vzer = CountVectorizer() #creates a CountVectorizer object\n",
        "v = vzer.fit_transform(doc) #fits the CountVectorizer object to the list of documents\n",
        "vzer.get_feature_names_out() #returns a list of the unique words that were found in the documents\n",
        "A=pd.DataFrame(v.toarray(),columns=vzer.get_feature_names_out()) #converting into dataframe to represent M number of unique words and N number of documents\n",
        "print(A) #prints the word document matrix 'A' with M number of unique words and N number of documents\n",
        "\n",
        "# Create a heatmap of the word-document matrix\n",
        "sns.heatmap(data=A,vmin=0,vmax=10,cmap='gray_r') #creates a heat map of 'A' with minimum value as 0 and maximum value as 10 with darker shades indicating higher frequency\n",
        "plt.show()\n",
        "\n",
        "#FINDING TF-IDF VALUE FOR EACH WORD\n",
        "\n",
        "vzer = TfidfVectorizer()  #creates a TfidfVectorizer object\n",
        "v = vzer.fit_transform(doc) #fits the TfidfVectorizer object to the list of documents\n",
        "vzer.get_feature_names_out() #returns a list of the unique words that were found in the documents\n",
        "print(v) #prints tf-idf values for each word.\n",
        "df = pd.DataFrame(v.toarray(), columns=vzer.get_feature_names_out()) #converting into dataframe for more readable format\n",
        "print(df) #prints the dataframe containing tf-idf value for each word from the document\n",
        "\n",
        "#USING COSINE SIMILARITY TO FIND 3 MOST SIMILAR DOCS TO \"10.TXT\"\n",
        "\n",
        "v = vzer.fit_transform(doc)\n",
        "# Create a cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(v, v)\n",
        "# Find the top 3 most similar documents to \"10.txt\"\n",
        "most_similar_docs = cosine_sim[9].argsort()[-4:][::-1] #here 10.txt is represented as 9 due to indices starting with 0.\n",
        "print(\"Top 3 most similar documents to 10.txt:\")\n",
        "print(most_similar_docs[1:]) #We get 5,18,1 which corresponds to \"6.txt\",\"19.txt\" and \"2.txt\" which are similar to \"10.txt\"\n",
        "\n",
        "#CREATING MATRIX B OF SIZE N x N, WHERE Bij REPRESENTS THE NUMBER OF COMMON WORDS BETWEEN DOC I AND DOC J\n",
        "\n",
        "# Get the number of documents\n",
        "n_docs = len(doc)\n",
        "# Create a matrix B of size N x N, where Bij will represent the number of common words between doc i and doc j.\n",
        "B = np.zeros((n_docs, n_docs))\n",
        "# For each pair of documents i and j, count the number of common words and set the corresponding elements of B\n",
        "for i in range(n_docs):\n",
        "  for j in range(i, n_docs):\n",
        "    words_str1 = doc[i].split() # Convert doc[i] and doc[j] into lists of words using str.split() function\n",
        "    words_str2 = doc[j].split()\n",
        "    common = [] #empty list 'common'\n",
        "    for word in words_str1:\n",
        "        # Loop through the words in words_str1\n",
        "        if words_str1== words_str2: #checks if both lists of words are the same.\n",
        "           common.append(words_str1)\n",
        "        elif word in words_str2 and word not in common:\n",
        "            # Append word to common if the word is in words_str2 as well.\n",
        "            # Only append if the word is not already in the common list.\n",
        "            common.append(word)\n",
        "    n_common_words=len(common) #number of common words is the length of 'common'\n",
        "    # Set the corresponding element of B\n",
        "    B[i, j] = n_common_words\n",
        "    B[j, i] = n_common_words\n",
        "\n",
        "# Print the matrix B\n",
        "print(B)\n"
      ]
    }
  ]
}